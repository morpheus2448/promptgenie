LLAMA_DIR="/home/morpheus/llama.cpp"
MODEL_DIR="$LLAMA_DIR/models/ggml"
#MODEL_FILE="dolphin-2_6-phi-2.Q4_0.gguf"
#MODEL_NAME="Dolphin 2.6 Phi-2 (Q4)"phi-2-orange.Q4_0.gguf
MODEL_FILE="phi-2-orange.Q4_0.gguf"
MODEL_NAME="Phi-2 Orange (Q4)"
MIN_P=0.9								# Default = 0.7
TEMP=0.1								# Default = 0.1
CTX_SIZE=0								# Default = 0 (load from model)
NUM_THREADS=4							# Default = 4 (best for small models)
REPEAT_PEN=1.2							# Default = 1.1
USE_GLOW=True							# https://github.com/charmbracelet/glow

if [ $USE_GLOW == True ]; then
	PROMPT="<|im_start|>system\nYou are a CLI assistant for GNU/Linux. Answer the given question in a concise manner. Format your responses clearly. Write in Markdown format.\n<|im_start|>user\n$QUERY\n<|im_start|>assistant\n"
else
	PROMPT="<|im_start|>system\nYou are a CLI assistant for GNU/Linux. Answer the given question in a concise manner. Always format responses in plain-text format.\n<|im_start|>user\n$QUERY\n<|im_start|>assistant\n"
fi

# NOTE: Delete ~/pg/cache.bin after changing model or context size!